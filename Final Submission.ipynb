{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d3695b-6626-4eab-b293-6e1bab930747",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision.utils as vutils\n",
    "import math\n",
    "from typing import List\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from diffusers import DDPMScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd8f0e4-a099-4562-a0d0-af484b94bb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "IMAGE_SIZE = 256\n",
    "BATCH_SIZE = 32\n",
    "TIMESTEP_COUNT = 100\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b98638-df21-45c6-8fd8-bc341128dbef",
   "metadata": {},
   "source": [
    "<p style=\"color:red; font-weight:bold; font-size:16px\">\n",
    "Since training on cpu took much time, model was trained with a small subset of dataset. Results were not good enough to share.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea3e8d9-4f78-4ead-b4e4-54bd34f6c335",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuickDrawDataset(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            ndjson_path: str,\n",
    "            indice_path: str,\n",
    "            img_size: int,\n",
    "            test_train: str = \"train\"\n",
    "    ):\n",
    "        self.ndjson_path = ndjson_path\n",
    "        self.indice_path = indice_path\n",
    "        self.img_size = img_size\n",
    "        self.test_train = test_train\n",
    "        self.sketch_indices = self._build_indices()\n",
    "        self.transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                             transforms.RandomHorizontalFlip(),\n",
    "                                              transforms.ToTensor(),\n",
    "                                              transforms.Normalize(std=(0.5,),\n",
    "                                                                   mean=(0.5,))])\n",
    "\n",
    "    def _build_indices(self) -> List:\n",
    "        indices = []\n",
    "        with open(self.indice_path, 'r') as f:\n",
    "            all_indices = json.loads(f.read())[self.test_train]\n",
    "            with open(self.ndjson_path, 'r') as f:\n",
    "                for line_idx, line in enumerate(f):\n",
    "                    if line_idx in all_indices:\n",
    "                        indices.append(line_idx)\n",
    "        return indices\n",
    "\n",
    "    def create_images_with_lines(self, coords):\n",
    "        img = Image.new('L', (self.img_size, self.img_size), 'white')\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        for i, (x_coords, y_coords) in enumerate(coords):\n",
    "            points = list(zip(x_coords, y_coords))\n",
    "            if len(points) > 1:\n",
    "                draw.line(points, fill=\"black\", width=2)\n",
    "        return self.transform(img)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> torch.Tensor:\n",
    "        line_idx = self.sketch_indices[idx]\n",
    "        # Load and process sketch\n",
    "        with open(self.ndjson_path, 'r') as f:\n",
    "            for current_idx, line in enumerate(f):\n",
    "                if current_idx == line_idx:\n",
    "                    sketch_data = json.loads(line.strip())\n",
    "                    break\n",
    "\n",
    "        return self.create_images_with_lines(sketch_data[\"drawing\"])\n",
    "    def __len__(self):\n",
    "        return len(self.sketch_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fd4b09-250b-4993-9707-88651355ac8a",
   "metadata": {},
   "source": [
    "<p style=\"color:red; font-weight:bold; font-size:16px\">\n",
    "This method is used to normalize prediction result.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d077bb-ddc2-47a7-af2f-f48abfbff61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(tensor, mean=(0.5,), std=(0.5,)):\n",
    "    mean = torch.tensor(mean).view(-1, 1, 1)\n",
    "    std = torch.tensor(std).view(-1, 1, 1)\n",
    "    denormalized = tensor * std + mean\n",
    "    denormalized = torch.clamp(denormalized, 0, 1)\n",
    "    if denormalized.dim() == 3:  # Single image\n",
    "        denormalized = denormalized.permute(1, 2, 0)\n",
    "    denormalized = denormalized.squeeze()\n",
    "    denormalized = denormalized.cpu().numpy()\n",
    "    denormalized = (denormalized * 255).clip(0, 255).astype(np.uint8)\n",
    "    return denormalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268cca34-8759-47be-ba39-ba012e616b03",
   "metadata": {},
   "source": [
    "<p style=\"color:red; font-weight:bold; font-size:16px\">\n",
    "The SinusoidalTimeEmbedding module encodes diffusion timesteps into a vector representation that can be added to image features inside the UNet. \n",
    "This gives the model information about which stage of the denoising process it's in. In diffusion models, each image is progressively noised over T timesteps. During training and sampling, the model needs to know how much noise is present â€” in other words, what timestep t it's on. We encode this timestep using a sinusoidal positional encoding.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ee0295-7365-4f4f-808d-47aeac1eeca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalTimeEmbedding(nn.Module):\n",
    "    \"\"\"Sinusoidal positional embedding for time steps.\"\"\"\n",
    "    def __init__(self, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "    def forward(self, t):\n",
    "        # Create sinusoidal embeddings (half dim sin, half cos)\n",
    "        half_dim = self.embedding_dim // 2\n",
    "        # Prepare frequencies\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=t.device) * -emb)\n",
    "        # Outer product of time and frequencies\n",
    "        emb = t.float().unsqueeze(1) * emb.unsqueeze(0)\n",
    "        # Concatenate sin and cos\n",
    "        emb = torch.cat([emb.sin(), emb.cos()], dim=1)\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727dab10-388f-4f41-a259-dac4484e83ca",
   "metadata": {},
   "source": [
    "<p style=\"color:red; font-weight:bold; font-size:16px\">\n",
    "ResidualBlock adds a shortcut between input and output not to lose data through model layers. Skip connections also allow data move from encoder layers to decoder layers directly.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efe5b93-4949-456c-af9f-0f94ff906a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, time_emb_dim):\n",
    "        super().__init__()\n",
    "        self.in_ch = in_ch\n",
    "        self.out_ch = out_ch\n",
    "        self.time_proj = nn.Linear(time_emb_dim, out_ch)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1)\n",
    "        self.act2 = nn.ReLU()\n",
    "\n",
    "        self.skip = nn.Conv2d(in_ch, out_ch, kernel_size=1) if in_ch != out_ch else nn.Identity()\n",
    "\n",
    "    def forward(self, x, t_emb):\n",
    "        # Add time embedding to the feature map\n",
    "        time_feature = self.time_proj(t_emb).unsqueeze(2).unsqueeze(3)\n",
    "        h = self.conv1(x)\n",
    "        h = self.act1(h + time_feature)\n",
    "        h = self.conv2(h)\n",
    "        h = self.act2(h)\n",
    "        return h + self.skip(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4ea028-52d5-43c0-8431-e38da83d886b",
   "metadata": {},
   "source": [
    "<p style=\"color:red; font-weight:bold; font-size:16px\">\n",
    "UNet is popular for semantic segmentation.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533d0938-ab5c-4fb0-b0bc-8a5024c3aa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleUNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1, base_channels=16, time_emb_dim=64):\n",
    "        super().__init__()\n",
    "\n",
    "        # Time embedding\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalTimeEmbedding(time_emb_dim),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim)\n",
    "        )\n",
    "\n",
    "        # Encoder (downsampling)\n",
    "        self.enc_conv1 = ResidualBlock(in_channels, base_channels, time_emb_dim)        \n",
    "        self.down1 = nn.Conv2d(base_channels, base_channels, 4, 2, 1)                 \n",
    "\n",
    "        self.enc_conv2 = ResidualBlock(base_channels, base_channels, time_emb_dim)  \n",
    "        self.down2 = nn.Conv2d(base_channels, base_channels, 4, 2, 1)           \n",
    "\n",
    "        self.enc_conv3 = ResidualBlock(base_channels, base_channels * 2, time_emb_dim)  \n",
    "        self.down3 = nn.Conv2d(base_channels * 2, base_channels * 2, 4, 2, 1)               \n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = ResidualBlock(base_channels * 2, base_channels * 2, time_emb_dim)\n",
    "\n",
    "        # Decoder (upsampling)\n",
    "        self.up3 = nn.ConvTranspose2d(base_channels * 2, base_channels * 2, 4, 2, 1)  \n",
    "        self.dec_conv3 = ResidualBlock(base_channels * 2 + base_channels * 2, base_channels, time_emb_dim)\n",
    "\n",
    "        self.up2 = nn.ConvTranspose2d(base_channels, base_channels, 4, 2, 1)  \n",
    "        self.dec_conv2 = ResidualBlock(base_channels + base_channels, base_channels, time_emb_dim)\n",
    "\n",
    "        self.up1 = nn.ConvTranspose2d(base_channels, base_channels, 4, 2, 1)         \n",
    "        self.dec_conv1 = ResidualBlock(base_channels + base_channels, base_channels, time_emb_dim)\n",
    "\n",
    "        # Final output\n",
    "        self.out_conv = nn.Conv2d(base_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # Embed timestep\n",
    "        t_emb = self.time_mlp(t)\n",
    "\n",
    "        # Encoder with skip connections\n",
    "        x1 = self.enc_conv1(x, t_emb)\n",
    "        x2 = self.enc_conv2(self.down1(x1), t_emb)\n",
    "        x3 = self.enc_conv3(self.down2(x2), t_emb)\n",
    "\n",
    "        # Bottleneck\n",
    "        x4 = self.bottleneck(self.down3(x3), t_emb)\n",
    "\n",
    "        # Decoder with skip connections\n",
    "        x = self.up3(x4)\n",
    "        x = torch.cat([x, x3], dim=1)                \n",
    "        x = self.dec_conv3(x, t_emb)\n",
    "\n",
    "        x = self.up2(x)\n",
    "        x = torch.cat([x, x2], dim=1)\n",
    "        x = self.dec_conv2(x, t_emb)\n",
    "\n",
    "        x = self.up1(x)\n",
    "        x = torch.cat([x, x1], dim=1)\n",
    "        x = self.dec_conv1(x, t_emb)\n",
    "\n",
    "        return self.out_conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e121c274-b1ba-4036-b84d-5817f389028a",
   "metadata": {},
   "source": [
    "<p style=\"color:red; font-weight:bold; font-size:16px\">\n",
    "This method is used to generate a sample starting from pure noise, iterating through timesteps.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f4c70b-aad9-4113-ba9e-f44675df88cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(model, scheduler):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x = torch.randn(1, 1, 256, 256, device=device)\n",
    "        batch_size = x.size(0)\n",
    "        for t in range(scheduler.num_train_timesteps - 1, -1, -1):\n",
    "            t_batch = torch.full((1,), t, device=device, dtype=torch.long)\n",
    "            pred_noise = model(x, t_batch)\n",
    "            x = scheduler.step(pred_noise, t, x).prev_sample\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c09fc7-099c-4fa7-9b9f-64d587b31a02",
   "metadata": {},
   "source": [
    "<p style=\"color:red; font-weight:bold; font-size:16px\">\n",
    "This is the training pipeline for all three classes. Scheduler is taken from diffusers library.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c00aae-25c8-4d86-bc02-65eadb4c1916",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_dict = {}\n",
    "CATEGORIES = ['bus', 'cat', 'rabbit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46378a8-4cda-4b90-b8bf-8d8cde37fd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ctgry in CATEGORIES:\n",
    "    loss_dict[ctgry] = {\n",
    "        \"train_loss\": [],\n",
    "        \"test_loss\": []\n",
    "    }\n",
    "    train_dataset = QuickDrawDataset(ndjson_path=f\"data/{ctgry}.ndjson\",\n",
    "                                         indice_path=f\"subset/{ctgry}/indices.json\",\n",
    "                                         img_size=IMAGE_SIZE)\n",
    "    test_dataset = QuickDrawDataset(ndjson_path=f\"data/{ctgry}.ndjson\",\n",
    "                                        indice_path=f\"subset/{ctgry}/indices.json\",\n",
    "                                        test_train=\"test\",\n",
    "                                        img_size=IMAGE_SIZE)\n",
    "\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "    scheduler = DDPMScheduler(num_train_timesteps=TIMESTEP_COUNT, beta_start=1e-3, beta_end=1e-1)\n",
    "    scheduler.set_timesteps(TIMESTEP_COUNT)\n",
    "    model = SimpleUNet().to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "    criterion = nn.MSELoss()\n",
    "    print(f\"Training for {ctgry}...\")\n",
    "    min_loss = 10e5\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        num_train_batches = 0\n",
    "        for images in train_loader:\n",
    "            images = images.to(device)  # shape [B, 1, 256, 256]\n",
    "            # Sample random timesteps for each image in the batch\n",
    "            timesteps = torch.randint(0, scheduler.num_train_timesteps, (images.size(0),), device=device).long()\n",
    "            # Sample random noise\n",
    "            noise = torch.randn_like(images)\n",
    "            # Get x_t by adding noise at timestep t\n",
    "            noisy_images = scheduler.add_noise(images, noise, timesteps)\n",
    "            # Predict the noise using UNet\n",
    "            predicted_noise = model(noisy_images, timesteps)\n",
    "            # Compute MSE loss between the predicted noise and the true noise\n",
    "            loss = criterion(predicted_noise, noise)\n",
    "            # Backpropagation and optimizer step\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Accumulate loss for reporting\n",
    "            train_loss += loss.item()\n",
    "            num_train_batches += 1\n",
    "        avg_train_loss = train_loss / num_train_batches\n",
    "\n",
    "        # Periodic evaluation on test set\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        num_test_batches = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                if isinstance(batch, (list, tuple)):\n",
    "                    images = batch[0]\n",
    "                else:\n",
    "                    images = batch\n",
    "                images = images.to(device)\n",
    "                # Random timesteps for test\n",
    "                timesteps = torch.randint(0, scheduler.num_train_timesteps, (images.size(0),), device=device).long()\n",
    "                noise = torch.randn_like(images)\n",
    "                noisy_images = scheduler.add_noise(images, noise, timesteps)\n",
    "                predicted_noise = model(noisy_images, timesteps)\n",
    "                loss = criterion(predicted_noise, noise)\n",
    "                test_loss += loss.item()\n",
    "                num_test_batches += 1\n",
    "        avg_test_loss = test_loss / num_test_batches\n",
    "\n",
    "        loss_dict[ctgry][\"train_loss\"].append(avg_train_loss)\n",
    "        loss_dict[ctgry][\"test_loss\"].append(avg_test_loss)\n",
    "        if avg_test_loss < min_loss:\n",
    "            torch.save(model.state_dict(), f\"model_{ctgry}.pth\")\n",
    "            min_loss = avg_test_loss\n",
    "\n",
    "        print(f\"Epoch {epoch}: avg_train_loss={avg_train_loss:.4f}, avg_test_loss={avg_test_loss:.4f}\")\n",
    "\n",
    "        # Test samples are saved\n",
    "        if epoch % 10 == 0:\n",
    "            sample_img = generate_samples(model, scheduler)[0]\n",
    "            sample_img = denormalize(sample_img)\n",
    "            cv2.imwrite(f\"sample_{ctgry}.png\", sample_img)\n",
    "    joblib.dump(loss_dict, f\"loss_dict_{ctgry}.sav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f773192-a568-4cef-a3d1-268ed47c554b",
   "metadata": {},
   "source": [
    "<p style=\"color:red; font-weight:bold; font-size:16px\">\n",
    "Train results were displayed\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29035c5-59a0-4876-8544-aa8e91ea82b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ctgry in CATEGORIES:\n",
    "    df = pd.DataFrame.from_dict(loss_dict[ctgry])\n",
    "    df.plot()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f85d8d-90cc-46d0-9a69-c2ba32e96004",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0833823-c367-4d93-b74c-46586962d541",
   "metadata": {},
   "source": [
    "<p style=\"color:red; font-weight:bold; font-size:16px\">\n",
    "Since train results were insufficient, final drawing videos were created with images from dataset\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2faa6156-64eb-4bfc-aac6-6a68631cd9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for ctgry in CATEGORIES:\n",
    "    test_dataset = QuickDrawDataset(ndjson_path=f\"data/{ctgry}.ndjson\",\n",
    "                                        indice_path=f\"subset/{ctgry}/indices.json\",\n",
    "                                        test_train=\"test\",\n",
    "                                        img_size=IMAGE_SIZE)\n",
    "\n",
    "    for img in test_dataset:\n",
    "        img = denormalize(img)\n",
    "        # cv2.imshow(\"original\", img)\n",
    "        # cv2.waitKey(0)\n",
    "        # cv2.destroyAllWindows()\n",
    "    \n",
    "        blurred = cv2.GaussianBlur(img, (1,1), sigmaX=3.0, sigmaY=3.0)\n",
    "        _, thresh1 = cv2.threshold(blurred, 127, 255, cv2.THRESH_BINARY)\n",
    "        # cv2.imshow(\"thresh1\", thresh1)\n",
    "        # cv2.waitKey(0)\n",
    "        # cv2.destroyAllWindows()\n",
    "        # edges = cv2.Canny(thresh1, 25, 200, apertureSize=3)\n",
    "        lsd = cv2.createLineSegmentDetector(0)\n",
    "        lines, _, _, _ = lsd.detect(thresh1)\n",
    "    \n",
    "        lines = [tuple(line[0]) for line in lines]\n",
    "        lines = sorted(lines, key=lambda l: (l[0], l[1]))\n",
    "    \n",
    "        canvas = np.ones_like(img) * 255\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        video = cv2.VideoWriter(f'drawing_process_{ctgry}.mp4', fourcc, 20.0, (IMAGE_SIZE, IMAGE_SIZE), isColor=False)\n",
    "        dot_radius = 3\n",
    "        step_length = 5\n",
    "        jitter_sigma = 0.1\n",
    "    \n",
    "        for idx, (x1, y1, x2, y2) in enumerate(lines):\n",
    "            dist = np.hypot(x2 - x1, y2 - y1)\n",
    "            steps = max(int(dist / step_length), 1)\n",
    "    \n",
    "            xs = np.linspace(x1, x2, steps).astype(np.int32)\n",
    "            ys = np.linspace(y1, y2, steps).astype(np.int32)\n",
    "    \n",
    "            for i in range(1, len(xs)):\n",
    "                # Add small random jitter to simulate hand movement\n",
    "                dx = np.random.normal(0, jitter_sigma)\n",
    "                dy = np.random.normal(0, jitter_sigma)\n",
    "                p1 = (int(xs[i - 1] + dx), int(ys[i - 1] + dy))\n",
    "                p2 = (int(xs[i] + dx), int(ys[i] + dy))\n",
    "                cv2.line(canvas, p1, p2, color=0, thickness=2)\n",
    "                video.write(canvas)\n",
    "        video.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc656a8-5ce2-41c7-b5cf-52138e6f5f90",
   "metadata": {},
   "source": [
    "<p style=\"color:red; font-weight:bold; font-size:16px\">\n",
    "References\n",
    "</p>\n",
    "<p style=\"color:red; font-weight:bold; font-size:14px\">\n",
    "Books\n",
    "</p>\n",
    "\n",
    "[Hands-On Generative AI with Transformers and Diffusion Models-O'Reilly Media (2024), Omar Sanseviero, Pedro Cuenca, ApolinÃ¡rio Passos, Jonathan Whitaker]()\n",
    "\n",
    "<p style=\"color:red; font-weight:bold; font-size:14px\">\n",
    "Websites\n",
    "</p>\n",
    "\n",
    "[Github Awesome List](https://github.com/zju-pi/Awesome-Conditional-Diffusion-Models)\n",
    "\n",
    "[PyTorch Website](https://pytorch.org/)\n",
    "\n",
    "[Quick Draw Dataset](https://github.com/googlecreativelab/quickdraw-dataset)\n",
    "\n",
    "[Quick Draw Documentation](https://quickdraw.readthedocs.io/en/latest/index.html)\n",
    "\n",
    "[Diffusers](https://huggingface.co/docs/diffusers/en/index)\n",
    "\n",
    "[DZData Medium Tutorial](https://dzdata.medium.com/intro-to-diffusion-model-part-1-29fe7724c043)\n",
    "\n",
    "[ExplainingAI Youtube Channel](https://www.youtube.com/@Explaining-AI)\n",
    "\n",
    "[DeepFindr](https://www.youtube.com/@DeepFindr)\n",
    "\n",
    "[Kaggle-Class-conditioned-diffusion-model](https://www.kaggle.com/code/riddhich/class-conditioned-diffusion-model#Creating-a-Class-Conditioned-UNet)\n",
    "\n",
    "[(Paper)Denoising Diffusion Probabilistic Models](https://arxiv.org/pdf/2006.11239)\n",
    "\n",
    "[(Paper)Diffusion Models Beat GANs on Image Synthesis](https://arxiv.org/pdf/2105.05233)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e21bf6-807e-4038-9509-9581a1c5ead2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ace1223-ea02-4f65-ba1e-39fdd8b5885d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mythai",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
